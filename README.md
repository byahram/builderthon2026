# BUILDERTHON2026
## Project Title  
**리워치 (Rewatch)**  
영상 속 원하는 순간을 AI가 찾아주는 RAG 기반 학습 & 복습 플랫폼

## intro
BUILDERTHON2026에서 개발한 프로젝트 '리워치(Rewatch)'는 영상 콘텐츠를 활용한 학습과 업무 효율성을 높이기 위한 RAG(Retrieval-Augmented Generation) 기반 AI 플랫폼입니다. 영상 콘텐츠는 교육과 업무에서 필수적인 도구로 자리 잡았지만, 필요한 정보를 찾기 위해 반복 시청해야 하는 비효율성이 큰 문제입니다. 예를 들어, 온라인 강의 플랫폼如 패스트캠퍼스 수강생들은 특정 코딩 스킬이나 툴 사용법을 확인하기 위해 긴 영상을 되감아 보아야 하며, 이는 시간 낭비로 이어집니다. 기업 환경에서도 업무 매뉴얼 영상의 특정 절차를 재확인하는 데 많은 시간이 소요됩니다. 또한, AI 챗봇의 답변은 환각(hallucination) 현상으로 인해 신뢰도가 낮아, 사용자가 원본 소스를 통해 검증하고 싶어도 텍스트 기반 답변만으로는 어려운 실정입니다. 이러한 문제를 해결하기 위해 리워치는 영상 내 특정 정보에 즉시 접근하고, 이를 체계적으로 복습·저장할 수 있는 솔루션을 제공합니다. GitHub에서 개발된 이 프로젝트는 96시간의 제한된 시간 내에 MVP(Minimum Viable Product)를 완성하였으며, AI 기술을 활용해 사용자 경험을 혁신적으로 개선하는 것을 목표로 합니다.

## method
프로젝트 개발은 팀 확정 후 96시간의 짧은 기간 내에 신속하게 진행되었습니다. 우선 GitHub 레포지토리를 새로 구축하고 협업 규칙을 수립하여 효율적인 개발 환경을 마련했습니다. 핵심 기술 스택은 다음과 같습니다:Core AI & LLM: OpenAI의 GPT-4o를 사용하여 사용자 질문의 의도를 파악하고, 추출된 문맥(Context)을 기반으로 최종 답변을 생성합니다. 프롬프트 엔지니어링을 통해 AI가 영상 콘텐츠에만 근거한 답변을 생성하도록 제한하여 환각 문제를 최소화했습니다.

Video & Audio Processing: yt-dlc를 통해 YouTube 영상을 MP3 파일로 추출한 후, OpenAI Whisper를 활용해 오디오를 텍스트로 변환합니다. 이 과정에서 타임스탬프 기반의 정밀한 스크립트를 생성하여 영상 구간과 텍스트를 연동합니다.

RAG Pipeline & Embeddings: OpenAI의 text-embedding-3 모델을 이용해 변환된 텍스트 청크(Chunk)와 사용자 쿼리를 고차원 벡터로 변환합니다. 이를 통해 시맨틱 유사도 검색을 구현하여 키워드 일치가 아닌 문맥적 유사성을 기반으로 관련 정보를 추출합니다.

Vector Database: Supabase(PostgreSQL / pgvector)를 벡터 저장소로 활용하며, match_documents 함수를 통해 고성능 시맨틱 검색 엔진을 구축했습니다.
Backend: Node.js를 사용해 서비스 로직을 처리하고, OpenAI 및 Supabase API를 연동합니다.
Frontend: Next.js와 TypeScript를 기반으로 안정적인 웹 애플리케이션을 개발하며, TailwindCSS를 통해 반응형 UI/UX를 구현합니다.

기술적 챌린지로는 비정형 데이터인 영상의 검색 기능성을 확보하는 것이었습니다. 이를 위해 영상을 오디오로 변환하고 Whisper로 텍스트화한 후, 단어 수 기반 청킹 알고리즘을 적용했습니다. 검색 정확도 문제를 해결하기 위해 단순 키워드 검색 대신 시맨틱 검색을 도입하였습니다. 예를 들어, 사용자가 "상태 관리"를 질문할 때 영상에서 "데이터 흐름 제어"로 표현되어도 문맥 유사성을 계산하여 정확한 구간을 추출합니다. RAG 워크플로우를 통해 타임스탬프 기반 청킹과 검색을 구현함으로써, AI 답변의 신뢰성을 높였습니다.

## result
리워치는 영상 콘텐츠 내 정보를 즉시 검색하고 복습할 수 있는 플랫폼으로, AI 챗봇이 영상 기반 답변과 타임스탬프를 제공합니다. 사용자는 AI 답변을 원본 영상으로 검증하며, 해당 구간을 즉시 재생할 수 있습니다. 핵심 기능인 '복습공간'은 모든 질문과 답변을 타임스탬프와 함께 자동 저장하여 개인 맞춤형 학습 노트를 생성합니다. 사용자 흐름은 다음과 같습니다: (1) AI 챗봇에 질문, (2) 답변과 타임스탬프 확인 후 구간 재생, (3) 복습공간에 자동 저장, (4) 저장된 구간만 재학습. 이로 인해 패스트캠퍼스 수강생처럼 특정 스킬을 효율적으로 복습할 수 있습니다. 96시간 내에 구현된 MVP는 기본적인 RAG 파이프라인과 UI를 완성하여, 영상 학습의 비효율성을 크게 줄이는 결과를 달성했습니다. GitHub 레포지토리에는 소스 코드와 문서가 공개되어 있으며, 데모를 통해 실제 동작을 확인할 수 있습니다.

## discussion
리워치의 MVP는 96시간의 제한된 준비 시간으로 인해 몇 가지 한계가 있습니다. 우선, 영상 처리 속도가 느려 대규모 영상이나 실시간 스트리밍을 지원하지 못합니다. Whisper와 embedding 모델의 처리 시간이 길어, 사용자 대기 시간이 발생할 수 있습니다. 또한, 복습공간의 소셜 기능(예: 동일 코스 수강생 간 질문 공유)은 계획 단계에 머물러 구현되지 않았습니다. 검색 정확도도 완벽하지 않아, 복잡한 문맥이나 다국어 영상에서 오류가 발생할 수 있으며, 벡터 데이터베이스의 스케일링이 부족합니다. 향후 발전 방향으로는 처리 속도를 최적화하기 위해 클라우드 기반 GPU 가속을 도입하고, 더 정교한 청킹 알고리즘(예: 문장 수준 청킹)을 적용할 수 있습니다. 
소셜 기능을 확장하여 커뮤니티 기반 학습을 지원하면, 가장 많이 질문된 문제의 집단 지식을 활용할 수 있습니다. 또한, 다양한 영상 소스(YouTube 외 Vimeo 등) 지원과 모바일 앱 개발을 통해 접근성을 높일 수 있습니다. 환각 문제를 더 강화하기 위해 멀티모달 AI 모델(예: GPT-4V)을 통합하여 영상 프레임 분석을 추가하면 검색 정확도가 향상될 것입니다. 궁극적으로, 리워치는 교육과 업무 분야에서 AI 기반 영상 학습의 표준 솔루션이 될 잠재력을 지니고 있습니다.
